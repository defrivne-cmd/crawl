{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl4AI: Scraping Python Documentation and Saving to Google Drive\n",
    "\n",
    "This notebook demonstrates how to use the `crawl4ai` library to scrape a website and save the results to your Google Drive. We will be targeting the official Python documentation at `https://docs.python.org`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive\n",
    "\n",
    "First, we need to mount your Google Drive to the Colab environment. This will allow us to save the scraped data directly to your drive. When you run the following cell, you will be prompted to authorize Colab to access your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "Next, we will install the `crawl4ai` library using pip. This library provides the tools we need to scrape the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"pyOpenSSL<=24.2.1\" crawl4ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Output Directory\n",
    "\n",
    "We will create a directory in your Google Drive to store the results of the crawl. The directory will be located at `/content/drive/MyDrive/crawlresults`. The code will check if the directory already exists and create it if it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_directory = '/content/drive/MyDrive/crawlresults'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure and Run the Scraper\n",
    "\n",
    "Now we will configure and run the `Crawl4AI` scraper. We will set the target URL to `https://docs.python.org` and specify the output directory we just created. The scraper will then crawl the website and save the content in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawl4ai import WebScraper\n",
    "\n",
    "scraper = WebScraper()\n",
    "scraper.run(\n",
    "    url='https://docs.python.org',\n",
    "    output_folder=output_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Once the scraper has finished, you can find the scraped data in the `crawlresults` folder in your Google Drive. The data will be organized in a way that is easy to process for further analysis or use in AI applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
